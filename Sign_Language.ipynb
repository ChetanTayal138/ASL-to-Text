{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sign-Language.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "nGSD2H5IcwmP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Importing all required libraries.\n",
        "\n",
        "import numpy as np  \n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2mUvnpu5lSwQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 126
        },
        "outputId": "fcdc1aeb-b8fd-42e6-c475-0d2a71e9b3e4"
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/gdrive') #Mounting our google drive on collab to directly use the dataset which we have manually uploaded to our google drive."
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "jWi0XJyQF9kK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "directory = 'gdrive/My Drive/Colab Notebooks'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8SuatESTGBQy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "c8d5be39-580a-4867-8948-14655d29e5c2"
      },
      "cell_type": "code",
      "source": [
        "%cd 'gdrive/My Drive/Colab Notebooks'"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/My Drive/Colab Notebooks\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "QTPsnSMxcwmd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Loading in our dataset\n",
        "training_df = pd.read_csv('sign_mnist_train.csv')\n",
        "testing_df  = pd.read_csv('sign_mnist_test.csv') \n",
        "training_df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "m92BdOCQcwmk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "training_df = training_df.values #Converting the pandas dataframe into numpy array"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3_X2vXZQcwmo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "outputId": "e7887c07-7501-4b6b-95b8-2b2c185d40a9"
      },
      "cell_type": "code",
      "source": [
        "training_df '''The images available in the dataset are 28x28 pixels. The dataset itself contains 27544 examples with each example having 784 features(28x28 = 784) and a label\n",
        "column to denote which class it belongs to.'''\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[  3, 107, 118, ..., 204, 203, 202],\n",
              "       [  6, 155, 157, ..., 103, 135, 149],\n",
              "       [  2, 187, 188, ..., 195, 194, 195],\n",
              "       ...,\n",
              "       [ 18, 174, 174, ..., 202, 200, 200],\n",
              "       [ 17, 177, 181, ...,  64,  87,  93],\n",
              "       [ 23, 179, 180, ..., 205, 209, 215]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "metadata": {
        "id": "rAYcAuY3cwmt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "x_training = training_df[: , 1:]\n",
        "y_training = training_df[: , 0] #Splitting the training numpy array into training_data and training_labels (our target values)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1laMRV_scwmw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "outputId": "7b289c66-7a7c-45f7-a21b-15814adcd72a"
      },
      "cell_type": "code",
      "source": [
        "x_training"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[107, 118, 127, ..., 204, 203, 202],\n",
              "       [155, 157, 156, ..., 103, 135, 149],\n",
              "       [187, 188, 188, ..., 195, 194, 195],\n",
              "       ...,\n",
              "       [174, 174, 174, ..., 202, 200, 200],\n",
              "       [177, 181, 184, ...,  64,  87,  93],\n",
              "       [179, 180, 180, ..., 205, 209, 215]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "metadata": {
        "id": "tDhIzxBecwm0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "8b687cb6-746a-403a-84ed-04c68e38dbbc"
      },
      "cell_type": "code",
      "source": [
        "y_training #The labels in our dataset are different categories which have been assigned to each letter.\n",
        "np.unique(y_training)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 0,  1,  2,  3,  4,  5,  6,  7,  8, 10, 11, 12, 13, 14, 15, 16, 17,\n",
              "       18, 19, 20, 21, 22, 23, 24])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "metadata": {
        "id": "3Fsw_yFTcwm4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "be2dee1c-56e3-4cf8-dff2-354c39c132de"
      },
      "cell_type": "code",
      "source": [
        "y_training.shape #To check we have all the available labels"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(27455,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "metadata": {
        "id": "r9IQLt2Icwm8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zRyC6vMMcwnB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "01b7f5ca-e2cf-4c03-fd0d-6a95e9008a0e"
      },
      "cell_type": "code",
      "source": [
        "x_training[0].shape"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(784,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "metadata": {
        "id": "oDs1Dk6UcwnF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "outputId": "5a1843a8-bdad-43fd-f405-776803e6d214"
      },
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(4,4))\n",
        "plt.imshow(x_training[7].reshape(28,28)) #Visualizing the first image available in our training dataset\n",
        "\n"
      ],
      "execution_count": 143,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7fa2da6f0be0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 143
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPoAAAD4CAYAAADFJPs2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n4D9OHqbpycjIZQAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7fa2da75fe80>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "lzOBX_rPcwna",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "69944b9f-aeb7-41d2-e5b9-7098b256b19e"
      },
      "cell_type": "code",
      "source": [
        "x_training.shape"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(27455, 784)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "metadata": {
        "id": "Knko4dnncwnn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "b14ccfea-d257-4315-c9a4-d00f252c2873"
      },
      "cell_type": "code",
      "source": [
        "y_training.shape"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(27455,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "metadata": {
        "id": "58Oazkf0cwnq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelBinarizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zYz1IY91cwnt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Labelising the given labels as binary. So each class will have a unique numpy array of 0s and 1s\n",
        "label_binarizer = LabelBinarizer() \n",
        "y_training = label_binarizer.fit_transform(y_training)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "eX50QSwCcwnw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "871baf2d-63aa-40a7-c5e3-535d9a8cd0cc"
      },
      "cell_type": "code",
      "source": [
        "y_training[7] #This particular example correspongs to the 22nd class which is letter X in the dataset"
      ],
      "execution_count": 144,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
              "       0, 0])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 144
        }
      ]
    },
    {
      "metadata": {
        "id": "iDvQjkzVcwn2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "x_training = x_training/255 #Normalizing the data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dqHW7R0vcwn5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "596f1ded-6271-4fbc-a0be-a5e0b51dedc4"
      },
      "cell_type": "code",
      "source": [
        "'''Our CNN will be accepting the inputs in (28x28x1) shape where 1 represents the 1 grayscale channel.Hence in this step we are reshaping all of the images to 28x28 \n",
        "so that these can be later fed into the network.'''\n",
        "\n",
        "\n",
        "x_training = np.array([np.reshape(i , (28,28,1)) for i in x_training])\n",
        "x_training.shape"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(27455, 28, 28, 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "metadata": {
        "id": "SqgwhXEjQjzH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "x_training "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NloThmcYcwoI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model = keras.Sequential([\n",
        "    \n",
        "    #LAYER 1\n",
        "    keras.layers.Conv2D(64 , kernel_size = (3,3) , activation='relu' , input_shape = (28,28,1)),\n",
        "    keras.layers.MaxPooling2D(pool_size=(2,2)),\n",
        "    \n",
        "    #LAYER 2\n",
        "    keras.layers.Conv2D(64 , kernel_size = (3,3) , activation='relu'),\n",
        "    keras.layers.MaxPooling2D(pool_size=(2,2)),\n",
        "    \n",
        "    #LAYER 3\n",
        "    keras.layers.Conv2D(64 , kernel_size = (3,3) , activation='relu'),\n",
        "    keras.layers.MaxPooling2D(pool_size=(2,2)),\n",
        "    \n",
        "    keras.layers.Flatten(),\n",
        "    #LAYER 4\n",
        "    keras.layers.Dense(128 , activation='relu'),\n",
        "    keras.layers.Dropout(0.20), #DropOut layer to prevent overfitting with 20% ratio\n",
        "    #LAYER 5\n",
        "    keras.layers.Dense(24 , activation='softmax') #Using softmax activation function because we have multiple classes to predict.\n",
        "    \n",
        "    \n",
        "    \n",
        "])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "oudtoE7WcwoN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model.compile(loss = ['categorical_crossentropy'] , \n",
        "              optimizer = tf.train.AdamOptimizer(), \n",
        "              metrics = ['accuracy']\n",
        "             )\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "iMHb3yWAcwoS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1812
        },
        "outputId": "b1c58da9-bbb6-43b8-cf51-08e2abb2e226"
      },
      "cell_type": "code",
      "source": [
        "history = model.fit(x_training , y_training , validation_split=0.2 , epochs = 50 , batch_size= 128) #While training the model we will use 20% of the training data for cross-validation.As you can see below we achieved very high accuracies for both the training and validation data."
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 21964 samples, validate on 5491 samples\n",
            "Epoch 1/50\n",
            "21964/21964 [==============================] - 4s 184us/step - loss: 2.7110 - acc: 0.1694 - val_loss: 1.8209 - val_acc: 0.4098\n",
            "Epoch 2/50\n",
            "21964/21964 [==============================] - 2s 103us/step - loss: 1.4939 - acc: 0.4999 - val_loss: 0.9421 - val_acc: 0.7115\n",
            "Epoch 3/50\n",
            "21964/21964 [==============================] - 2s 104us/step - loss: 0.8667 - acc: 0.7076 - val_loss: 0.6103 - val_acc: 0.8062\n",
            "Epoch 4/50\n",
            "21964/21964 [==============================] - 2s 104us/step - loss: 0.5674 - acc: 0.8101 - val_loss: 0.3439 - val_acc: 0.9007\n",
            "Epoch 5/50\n",
            "21964/21964 [==============================] - 2s 104us/step - loss: 0.3828 - acc: 0.8750 - val_loss: 0.2270 - val_acc: 0.9368\n",
            "Epoch 6/50\n",
            "21964/21964 [==============================] - 2s 103us/step - loss: 0.2637 - acc: 0.9158 - val_loss: 0.1475 - val_acc: 0.9581\n",
            "Epoch 7/50\n",
            "21964/21964 [==============================] - 2s 104us/step - loss: 0.1969 - acc: 0.9370 - val_loss: 0.1069 - val_acc: 0.9738\n",
            "Epoch 8/50\n",
            "21964/21964 [==============================] - 2s 103us/step - loss: 0.1436 - acc: 0.9535 - val_loss: 0.0717 - val_acc: 0.9843\n",
            "Epoch 9/50\n",
            "21964/21964 [==============================] - 2s 103us/step - loss: 0.0996 - acc: 0.9703 - val_loss: 0.0419 - val_acc: 0.9925\n",
            "Epoch 10/50\n",
            "21964/21964 [==============================] - 2s 104us/step - loss: 0.0809 - acc: 0.9761 - val_loss: 0.0344 - val_acc: 0.9914\n",
            "Epoch 11/50\n",
            "21964/21964 [==============================] - 2s 102us/step - loss: 0.0640 - acc: 0.9820 - val_loss: 0.0192 - val_acc: 0.9973\n",
            "Epoch 12/50\n",
            "21964/21964 [==============================] - 2s 103us/step - loss: 0.0500 - acc: 0.9861 - val_loss: 0.0253 - val_acc: 0.9944\n",
            "Epoch 13/50\n",
            "21964/21964 [==============================] - 2s 102us/step - loss: 0.0439 - acc: 0.9880 - val_loss: 0.0160 - val_acc: 0.9965\n",
            "Epoch 14/50\n",
            "21964/21964 [==============================] - 2s 100us/step - loss: 0.0348 - acc: 0.9905 - val_loss: 0.0091 - val_acc: 0.9984\n",
            "Epoch 15/50\n",
            "21964/21964 [==============================] - 2s 100us/step - loss: 0.0277 - acc: 0.9931 - val_loss: 0.0100 - val_acc: 0.9982\n",
            "Epoch 16/50\n",
            "21964/21964 [==============================] - 2s 100us/step - loss: 0.0270 - acc: 0.9922 - val_loss: 0.0100 - val_acc: 0.9985\n",
            "Epoch 17/50\n",
            "21964/21964 [==============================] - 2s 100us/step - loss: 0.0254 - acc: 0.9927 - val_loss: 0.0081 - val_acc: 0.9982\n",
            "Epoch 18/50\n",
            "21964/21964 [==============================] - 2s 100us/step - loss: 0.0197 - acc: 0.9944 - val_loss: 0.0051 - val_acc: 0.9989\n",
            "Epoch 19/50\n",
            "21964/21964 [==============================] - 2s 99us/step - loss: 0.0226 - acc: 0.9939 - val_loss: 0.0069 - val_acc: 0.9985\n",
            "Epoch 20/50\n",
            "21964/21964 [==============================] - 2s 99us/step - loss: 0.0169 - acc: 0.9952 - val_loss: 0.0047 - val_acc: 0.9989\n",
            "Epoch 21/50\n",
            "21964/21964 [==============================] - 2s 100us/step - loss: 0.0170 - acc: 0.9951 - val_loss: 0.0054 - val_acc: 0.9985\n",
            "Epoch 22/50\n",
            "21964/21964 [==============================] - 2s 100us/step - loss: 0.0150 - acc: 0.9957 - val_loss: 0.0040 - val_acc: 0.9995\n",
            "Epoch 23/50\n",
            "21964/21964 [==============================] - 2s 100us/step - loss: 0.0111 - acc: 0.9972 - val_loss: 0.0024 - val_acc: 0.9995\n",
            "Epoch 24/50\n",
            "21964/21964 [==============================] - 2s 100us/step - loss: 0.0137 - acc: 0.9961 - val_loss: 0.0014 - val_acc: 0.9998\n",
            "Epoch 25/50\n",
            "21964/21964 [==============================] - 2s 100us/step - loss: 0.0168 - acc: 0.9950 - val_loss: 0.0172 - val_acc: 0.9936\n",
            "Epoch 26/50\n",
            "21964/21964 [==============================] - 2s 99us/step - loss: 0.0161 - acc: 0.9953 - val_loss: 0.0045 - val_acc: 0.9991\n",
            "Epoch 27/50\n",
            "21964/21964 [==============================] - 2s 100us/step - loss: 0.0095 - acc: 0.9975 - val_loss: 0.0015 - val_acc: 0.9996\n",
            "Epoch 28/50\n",
            "21964/21964 [==============================] - 2s 100us/step - loss: 0.0099 - acc: 0.9969 - val_loss: 0.0026 - val_acc: 0.9996\n",
            "Epoch 29/50\n",
            "21964/21964 [==============================] - 2s 100us/step - loss: 0.0133 - acc: 0.9954 - val_loss: 8.6287e-04 - val_acc: 0.9998\n",
            "Epoch 30/50\n",
            "21964/21964 [==============================] - 2s 100us/step - loss: 0.0077 - acc: 0.9977 - val_loss: 9.2556e-04 - val_acc: 1.0000\n",
            "Epoch 31/50\n",
            "21964/21964 [==============================] - 2s 99us/step - loss: 0.0107 - acc: 0.9971 - val_loss: 0.0201 - val_acc: 0.9924\n",
            "Epoch 32/50\n",
            "21964/21964 [==============================] - 2s 100us/step - loss: 0.0104 - acc: 0.9969 - val_loss: 9.9634e-04 - val_acc: 0.9996\n",
            "Epoch 33/50\n",
            "21964/21964 [==============================] - 2s 99us/step - loss: 0.0047 - acc: 0.9987 - val_loss: 5.1369e-04 - val_acc: 0.9998\n",
            "Epoch 34/50\n",
            "21964/21964 [==============================] - 2s 99us/step - loss: 0.0060 - acc: 0.9982 - val_loss: 0.0022 - val_acc: 0.9993\n",
            "Epoch 35/50\n",
            "21964/21964 [==============================] - 2s 100us/step - loss: 0.0141 - acc: 0.9956 - val_loss: 0.0016 - val_acc: 0.9998\n",
            "Epoch 36/50\n",
            "21964/21964 [==============================] - 2s 99us/step - loss: 0.0066 - acc: 0.9980 - val_loss: 5.7628e-04 - val_acc: 0.9998\n",
            "Epoch 37/50\n",
            "21964/21964 [==============================] - 2s 100us/step - loss: 0.0068 - acc: 0.9980 - val_loss: 0.0020 - val_acc: 0.9993\n",
            "Epoch 38/50\n",
            "21964/21964 [==============================] - 2s 100us/step - loss: 0.0139 - acc: 0.9955 - val_loss: 0.0014 - val_acc: 0.9996\n",
            "Epoch 39/50\n",
            "21964/21964 [==============================] - 2s 101us/step - loss: 0.0045 - acc: 0.9988 - val_loss: 2.3590e-04 - val_acc: 1.0000\n",
            "Epoch 40/50\n",
            "21964/21964 [==============================] - 2s 101us/step - loss: 0.0015 - acc: 0.9998 - val_loss: 3.1380e-04 - val_acc: 1.0000\n",
            "Epoch 41/50\n",
            "21964/21964 [==============================] - 2s 100us/step - loss: 0.0053 - acc: 0.9986 - val_loss: 0.0011 - val_acc: 0.9998\n",
            "Epoch 42/50\n",
            "21964/21964 [==============================] - 2s 99us/step - loss: 0.0034 - acc: 0.9990 - val_loss: 2.5932e-04 - val_acc: 1.0000\n",
            "Epoch 43/50\n",
            "21964/21964 [==============================] - 2s 100us/step - loss: 0.0070 - acc: 0.9977 - val_loss: 0.0054 - val_acc: 0.9985\n",
            "Epoch 44/50\n",
            "21964/21964 [==============================] - 2s 100us/step - loss: 0.0154 - acc: 0.9951 - val_loss: 8.7143e-04 - val_acc: 1.0000\n",
            "Epoch 45/50\n",
            "21964/21964 [==============================] - 2s 101us/step - loss: 0.0075 - acc: 0.9971 - val_loss: 4.9772e-04 - val_acc: 1.0000\n",
            "Epoch 46/50\n",
            "21964/21964 [==============================] - 2s 101us/step - loss: 0.0056 - acc: 0.9985 - val_loss: 0.0012 - val_acc: 1.0000\n",
            "Epoch 47/50\n",
            "21964/21964 [==============================] - 2s 101us/step - loss: 0.0069 - acc: 0.9977 - val_loss: 4.2848e-04 - val_acc: 0.9998\n",
            "Epoch 48/50\n",
            "21964/21964 [==============================] - 2s 102us/step - loss: 0.0037 - acc: 0.9989 - val_loss: 6.5169e-04 - val_acc: 1.0000\n",
            "Epoch 49/50\n",
            "21964/21964 [==============================] - 2s 102us/step - loss: 0.0055 - acc: 0.9984 - val_loss: 0.0026 - val_acc: 0.9993\n",
            "Epoch 50/50\n",
            "21964/21964 [==============================] - 2s 101us/step - loss: 0.0027 - acc: 0.9991 - val_loss: 2.8396e-04 - val_acc: 1.0000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "cxl0Dw_BSXQP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 223
        },
        "outputId": "51a50171-2827-4afe-aea4-199f83e1dedf"
      },
      "cell_type": "code",
      "source": [
        "testing_df.head(5)"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>pixel1</th>\n",
              "      <th>pixel2</th>\n",
              "      <th>pixel3</th>\n",
              "      <th>pixel4</th>\n",
              "      <th>pixel5</th>\n",
              "      <th>pixel6</th>\n",
              "      <th>pixel7</th>\n",
              "      <th>pixel8</th>\n",
              "      <th>pixel9</th>\n",
              "      <th>...</th>\n",
              "      <th>pixel775</th>\n",
              "      <th>pixel776</th>\n",
              "      <th>pixel777</th>\n",
              "      <th>pixel778</th>\n",
              "      <th>pixel779</th>\n",
              "      <th>pixel780</th>\n",
              "      <th>pixel781</th>\n",
              "      <th>pixel782</th>\n",
              "      <th>pixel783</th>\n",
              "      <th>pixel784</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>6</td>\n",
              "      <td>149</td>\n",
              "      <td>149</td>\n",
              "      <td>150</td>\n",
              "      <td>150</td>\n",
              "      <td>150</td>\n",
              "      <td>151</td>\n",
              "      <td>151</td>\n",
              "      <td>150</td>\n",
              "      <td>151</td>\n",
              "      <td>...</td>\n",
              "      <td>138</td>\n",
              "      <td>148</td>\n",
              "      <td>127</td>\n",
              "      <td>89</td>\n",
              "      <td>82</td>\n",
              "      <td>96</td>\n",
              "      <td>106</td>\n",
              "      <td>112</td>\n",
              "      <td>120</td>\n",
              "      <td>107</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>5</td>\n",
              "      <td>126</td>\n",
              "      <td>128</td>\n",
              "      <td>131</td>\n",
              "      <td>132</td>\n",
              "      <td>133</td>\n",
              "      <td>134</td>\n",
              "      <td>135</td>\n",
              "      <td>135</td>\n",
              "      <td>136</td>\n",
              "      <td>...</td>\n",
              "      <td>47</td>\n",
              "      <td>104</td>\n",
              "      <td>194</td>\n",
              "      <td>183</td>\n",
              "      <td>186</td>\n",
              "      <td>184</td>\n",
              "      <td>184</td>\n",
              "      <td>184</td>\n",
              "      <td>182</td>\n",
              "      <td>180</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>10</td>\n",
              "      <td>85</td>\n",
              "      <td>88</td>\n",
              "      <td>92</td>\n",
              "      <td>96</td>\n",
              "      <td>105</td>\n",
              "      <td>123</td>\n",
              "      <td>135</td>\n",
              "      <td>143</td>\n",
              "      <td>147</td>\n",
              "      <td>...</td>\n",
              "      <td>68</td>\n",
              "      <td>166</td>\n",
              "      <td>242</td>\n",
              "      <td>227</td>\n",
              "      <td>230</td>\n",
              "      <td>227</td>\n",
              "      <td>226</td>\n",
              "      <td>225</td>\n",
              "      <td>224</td>\n",
              "      <td>222</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>203</td>\n",
              "      <td>205</td>\n",
              "      <td>207</td>\n",
              "      <td>206</td>\n",
              "      <td>207</td>\n",
              "      <td>209</td>\n",
              "      <td>210</td>\n",
              "      <td>209</td>\n",
              "      <td>210</td>\n",
              "      <td>...</td>\n",
              "      <td>154</td>\n",
              "      <td>248</td>\n",
              "      <td>247</td>\n",
              "      <td>248</td>\n",
              "      <td>253</td>\n",
              "      <td>236</td>\n",
              "      <td>230</td>\n",
              "      <td>240</td>\n",
              "      <td>253</td>\n",
              "      <td>255</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>3</td>\n",
              "      <td>188</td>\n",
              "      <td>191</td>\n",
              "      <td>193</td>\n",
              "      <td>195</td>\n",
              "      <td>199</td>\n",
              "      <td>201</td>\n",
              "      <td>202</td>\n",
              "      <td>203</td>\n",
              "      <td>203</td>\n",
              "      <td>...</td>\n",
              "      <td>26</td>\n",
              "      <td>40</td>\n",
              "      <td>64</td>\n",
              "      <td>48</td>\n",
              "      <td>29</td>\n",
              "      <td>46</td>\n",
              "      <td>49</td>\n",
              "      <td>46</td>\n",
              "      <td>46</td>\n",
              "      <td>53</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 785 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   label  pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  pixel8  \\\n",
              "0      6     149     149     150     150     150     151     151     150   \n",
              "1      5     126     128     131     132     133     134     135     135   \n",
              "2     10      85      88      92      96     105     123     135     143   \n",
              "3      0     203     205     207     206     207     209     210     209   \n",
              "4      3     188     191     193     195     199     201     202     203   \n",
              "\n",
              "   pixel9    ...     pixel775  pixel776  pixel777  pixel778  pixel779  \\\n",
              "0     151    ...          138       148       127        89        82   \n",
              "1     136    ...           47       104       194       183       186   \n",
              "2     147    ...           68       166       242       227       230   \n",
              "3     210    ...          154       248       247       248       253   \n",
              "4     203    ...           26        40        64        48        29   \n",
              "\n",
              "   pixel780  pixel781  pixel782  pixel783  pixel784  \n",
              "0        96       106       112       120       107  \n",
              "1       184       184       184       182       180  \n",
              "2       227       226       225       224       222  \n",
              "3       236       230       240       253       255  \n",
              "4        46        49        46        46        53  \n",
              "\n",
              "[5 rows x 785 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "metadata": {
        "id": "IJjJe7kASjVP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "testing_df = testing_df.values #Converting the pandas dataframe into numpy arrays."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dpLRQQ2cSjoW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "x_test = training_df[: , 1:]\n",
        "y_test = training_df[: , 0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "aFNKRuAdUqGI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "label_binarizer = LabelBinarizer()\n",
        "y_test = label_binarizer.fit_transform(y_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hRubwGCvSj3P",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "2230641a-ada0-438d-89dd-12262c54c79d"
      },
      "cell_type": "code",
      "source": [
        "x_test[0].shape"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(784,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "metadata": {
        "id": "CBY-v75eSkLG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "x_test = np.array([np.reshape(i , (28,28,1)) for i in x_test])\n",
        "x_test"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "V-6AY4V9cwoU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "edfd51ba-48e9-4af8-a12f-37ef1fb15771"
      },
      "cell_type": "code",
      "source": [
        "x_test.shape"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(27455, 28, 28, 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "metadata": {
        "id": "At9ux0bGTtAh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vW9zIItNcwoX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "y_predicitions = model.predict(x_test) #Prediction on to the test set\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QrgJ0d4_cwob",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "abb8e5a4-5332-4a22-fcc8-6106f897f5b1"
      },
      "cell_type": "code",
      "source": [
        "accuracy_score(y_test , y_predicitions.round()) #Finally we achieved a testing accuracy of about 97.9% on the test set."
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9795301402294664"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        }
      ]
    },
    {
      "metadata": {
        "id": "q2mWQTDAcwoe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "78d9ee96-2e81-4a9c-8c59-36a36fe5671a"
      },
      "cell_type": "code",
      "source": [
        "#Downloading our model to be used in the opencv module.\n",
        "\n",
        "from keras.models import model_from_json\n",
        "model_json = model.to_json()\n",
        "with open(\"model.json\", \"w\") as json_file:\n",
        "    json_file.write(model_json)\n",
        "# serialize weights to HDF5\n",
        "model.save_weights(\"model.h5\")\n",
        "print(\"Saved model to disk\") "
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Saved model to disk\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "P8D4m-y6cwoi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download('model.h5')#Downloading the weights of our trained model\n",
        "files.download('model.json')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Pbi7tvhmildx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 497
        },
        "outputId": "357b59fc-3de8-4834-dc12-f248627c7118"
      },
      "cell_type": "code",
      "source": [
        "model.summary() #A final summary of the model."
      ],
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d (Conv2D)              (None, 26, 26, 64)        640       \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 13, 13, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 11, 11, 64)        36928     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 5, 5, 64)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 3, 3, 64)          36928     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 1, 1, 64)          0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 128)               8320      \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 24)                3096      \n",
            "=================================================================\n",
            "Total params: 85,912\n",
            "Trainable params: 85,912\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}